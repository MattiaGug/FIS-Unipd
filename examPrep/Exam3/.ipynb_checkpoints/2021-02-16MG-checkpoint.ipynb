{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2ef7ed69c33af452a7bc189ce7d3aba2",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 2021-02-16 Exam\n",
    "\n",
    "### General Instructions:\n",
    "\n",
    "Welcome to the **Python Programming (for Data Science)** exam session! Please, read **carefully** the instructions below before start writing code. \n",
    "\n",
    "This session will last **75 minutes** and is divided into **two parts**: one about \"general\" Python programming and the other about Python programming for Data Science. Each part is made of a set of exercises, which globally accounts for **15** + **15** = **30 points**.\n",
    "You will earn all of the points associated to an exercise **if and only if** the answer you provide passes successfully **all** the tests (both those that are visible and those that are hidden to you).\n",
    "At the end of these two parts, there is a part related to Machine Learning which can be considered a bonus for the students of the Cohort 2019-2020 who will achieve at least a score of 22.<br />\n",
    "\n",
    "To actually write down your implementation, make sure to fill in any place that says <code style=\"color:green\">**_# YOUR CODE HERE_**</code>. Note also that you should **either comment or delete** any <code style=\"color:green\">**raise NotImplementedError()**</code> exception.<br />\n",
    "\n",
    "For this exam session **you will not be allowed** to use any lecture material yet you will be able to access the following APIs:\n",
    "\n",
    "-  [Python](https://docs.python.org/)\n",
    "-  [Numpy](https://numpy.org/)\n",
    "-  [Scipy](https://docs.scipy.org/)\n",
    "-  [Pandas](https://pandas.pydata.org/)\n",
    "-  [Matplotlib](https://matplotlib.org/)\n",
    "-  [Seaborn](http://seaborn.pydata.org/)\n",
    "-  [SciKit-Learn](http://scikit-learn.org/)\n",
    "\n",
    "Once you are done, save this notebook and rename it as follows:\n",
    "\n",
    "<code>**YOURUSERNAME_2021-02-16.ipynb**</code>\n",
    "\n",
    "where <code>**YOURUSERNAME**</code> is your actual username. To be consistent, we are expecting your username to be composed by your first name's initial, followed by your full lastname. As an example, in my case this notebook must be saved as <code>**gdinunzio_2021-02-16.ipynb**</code> (Remember to insert an underscore <code>**'_'**</code> between your username and the date).<br />\n",
    "\n",
    "Finally, go back to the [Moodle](https://esami.elearning.unipd.it/)\n",
    ") web page of the \"**2021-02-16 Python Programming Exam**\"; there, you will be able to upload your notebook file for grading.\n",
    "\n",
    "<center><h3>Submissions are allowed until <span style=\"color:red\">Tuesday, 16th February 2021 at 11:15 AM</span></h3></center>\n",
    "\n",
    "Note that there is no limit on the number of submissions; however, be careful when you upload a new version of this notebook because each submission overwrites the previous one. \n",
    "The due date indicated above is **strict**; after that, the system will not accept any more submissions and the latest uploaded notebook will be the one considered for grading.\n",
    "\n",
    "The archive you have downloaded (<code style=\"color:magenta\">**2021-02-16-exam.zip**</code>) is orgaized as follows:\n",
    "\n",
    "<code style=\"color:red\">**2021-02-16-exam**</code> (root)<br />\n",
    "|----<code style=\"color:green\">**2020-02-16.ipynb**</code> (_this_ notebook)<br />\n",
    "|----<code>**dataset_notype.csv**</code> (the dataset you will be using for answering data science related questions)<br />\n",
    "|----<code>**database_type.sqlite**</code> (the database you will be using for answering data science related questions)<br />\n",
    "|----<code>**README.txt**</code> (a description of the dataset above)\n",
    "\n",
    "<center><h3>... Now, sit back, relax, and do your best!</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Name** = *Ngoc Diem*\n",
    "\n",
    "**Last Name** = *Le*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e378f1ee694ed10b309a2dfba179439d",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Adding the following line, allows Jupyter Notebook to visualize plots\n",
    "# produced by matplotlib directly below the code cell which generated those.\n",
    "#%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nose.tools import assert_equal\n",
    "from operator import itemgetter\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "EPSILON = .0000001 # tiny tolerance for managing subtle differences resulting from floating point operations\n",
    "\n",
    "DATASET_FILE = \"dataset_notype.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9abf62ed3b37fcecc351201bc408247e",
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: General Coding (15 points)\n",
    "\n",
    "For **Part 1**, you will be asked to use the dictionary below - called <code>**friends**</code> - which represents a small social network graph extracted from **10** characters of the _Friends_ TV series. Each entry of this dictionary contains the name of the character as key and a list of tuples as value; each tuple, in turn, is composed of two items: the name of a character and a float in the range **(0, 1]** which measures the strength of this relationship.<br /> \n",
    "Please, execute the cell right below and answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e4e9e29f608d0d32add61f85345d21ce",
     "grade": false,
     "grade_id": "part-1-required",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "friends = {\n",
    "    'Rachel Green': [('Ross Geller', 1.0), \n",
    "                     ('Paul Stevens', 0.46),\n",
    "                     ('Chandler Bing', 0.73),\n",
    "                     ('Monica Geller', 0.92),\n",
    "                     ('Phoebe Buffay', 0.94),\n",
    "                     ('Joey Tribbiani', 0.97),\n",
    "                     ('Gunther', 0.01)\n",
    "                    ],\n",
    "    'Monica Geller': [('Ross Geller', 0.85),\n",
    "                      ('Phoebe Buffay', 0.89),\n",
    "                      ('Rachel Green', 0.89),\n",
    "                      ('Chandler Bing', 1.0),\n",
    "                      ('Joey Tribbiani', 0.90),\n",
    "                      ('Janice Hosenstein', 0.07),\n",
    "                      ('Gunther', 0.51),\n",
    "                      ('Ursula Buffay', 0.42),\n",
    "                      ('Paul Stevens', 0.27)\n",
    "                     ],\n",
    "    'Phoebe Buffay': [('Ursula Buffay', 0.01),\n",
    "                      ('Joey Tribbiani', 0.96),\n",
    "                      ('Monica Geller', 0.81),\n",
    "                      ('Rachel Green', 0.88),\n",
    "                      ('Chandler Bing', 0.64),\n",
    "                      ('Ross Geller', 0.83),\n",
    "                      ('Gunther', 0.25)\n",
    "                     ],\n",
    "    'Joey Tribbiani': [('Phoebe Buffay', 0.98),\n",
    "                       ('Chandler Bing', 1.0),\n",
    "                       ('Ross Geller', 0.99),\n",
    "                       ('Rachel Green', 0.97),\n",
    "                       ('Monica Geller', 0.95)\n",
    "                      ],\n",
    "    'Chandler Bing': [('Monica Geller', 1.0),\n",
    "                      ('Joey Tribbiani', 1.0),\n",
    "                      ('Ross Geller', 0.97),\n",
    "                      ('Phoebe Buffay', 0.81),\n",
    "                      ('Rachel Green', 0.68),\n",
    "                      ('Gunther', 0.37),\n",
    "                      ('Janice Hosenstein', 0.16)\n",
    "                     ],\n",
    "    'Ross Geller': [('Monica Geller', 0.95),\n",
    "                    ('Joey Tribbiani', 0.96),\n",
    "                    ('Chandler Bing', 0.96),\n",
    "                    ('Phoebe Buffay', 0.91),\n",
    "                    ('Rachel Green', 1.0),\n",
    "                    ('Paul Stevens', 0.04)],\n",
    "    'Gunther': [('Rachel Green', 1.0)],\n",
    "    'Ursula Buffay': [],\n",
    "    'Janice Hosenstein': [('Chandler Bing', 1.0),\n",
    "                          ('Monica Geller', 0.01),\n",
    "                          ('Joey Tribbiani', 0.32)\n",
    "                         ],\n",
    "    'Paul Stevens': [('Rachel Green', 0.88),\n",
    "                     ('Ross Geller', 0.29),\n",
    "                     ('Monica Geller', 0.57)\n",
    "                    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "db6dcec0ffadf27f18adfc178b6de769",
     "grade": false,
     "grade_id": "exercise-1-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.1 (1 point)\n",
    "\n",
    "Implement the function <code><b>n_friends_of</b></code>, which takes as input a string (i.e., a character) <code><b>u</b></code> and returns the total number of friends of <code><b>u</b></code>, or <code><b>-1</b></code> if <code><b>u</b></code> is not part of the <code><b>friends</b></code> social network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eca464d5a7b3be6a47d4ed63bf7d2836",
     "grade": false,
     "grade_id": "exercise-1-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_friends_of(u):\n",
    "    \"\"\"\n",
    "    Return the total number of u's friends or -1 if u is not part of the `friends` network.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    if u not in friends:\n",
    "        return -1\n",
    "    else:\n",
    "        return len(friends[u])\n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "n_friends_of('Phoebe Buffay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4ccc95394e22f90b2c4ab55a8745677e",
     "grade": true,
     "grade_id": "exercise-1-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `n_friends_of` function\n",
    "\"\"\"\n",
    "# save the original `friends` social network graph \n",
    "# (redundant operation so as to be sure everything will be ok if the original `friends` gets accidentally modified)\n",
    "old_friends = copy.deepcopy(friends)\n",
    "\n",
    "# Tests\n",
    "assert_equal(6, n_friends_of(\"Ross Geller\"))\n",
    "assert_equal(7, n_friends_of(\"Phoebe Buffay\"))\n",
    "assert_equal(False, n_friends_of(\"Ursula Buffay\") > 1)\n",
    "\n",
    "# revert to the original `friends` social network graph\n",
    "friends = old_friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4fc57779077201f2ac8b64a0e4f289c6",
     "grade": false,
     "grade_id": "exercise-1-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.2 (3 points)\n",
    "\n",
    "Implement the function <code><b>avg_friend_score_map</b></code>, which builds a <b>new</b> dictionary out of the given <code><b>friends</b></code> as follows: for each entry of the original <code><b>friends</b></code> (i.e., character) it creates a new entry on the new dictionary, whose key is the character's name (i.e., the key of the original dictionary) and whose value is the <b>average friendship score</b> of that character <b>providing that character has AT LEAST ONE friend</b> (otherwise, no entry will be created for that character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rachel Green': 0.7185714285714285,\n",
       " 'Monica Geller': 0.6444444444444444,\n",
       " 'Phoebe Buffay': 0.6257142857142857,\n",
       " 'Joey Tribbiani': 0.978,\n",
       " 'Chandler Bing': 0.7128571428571429,\n",
       " 'Ross Geller': 0.8033333333333333,\n",
       " 'Gunther': 1.0,\n",
       " 'Janice Hosenstein': 0.44333333333333336,\n",
       " 'Paul Stevens': 0.58}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_friend_score_map():\n",
    "    avg_friend_score_map = {}\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    for char in friends.keys():\n",
    "        if len(friends[char]) != 0:\n",
    "            score = []\n",
    "            for i in range(len(friends[char])):\n",
    "                score.append(friends[char][i][1])\n",
    "            avg_friend_score_map[char] = np.mean(score)\n",
    "    return avg_friend_score_map\n",
    "        \n",
    "avg_friend_score_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c27ac7bfa126dd0554fb53afc5776dcb",
     "grade": false,
     "grade_id": "exercise-1-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d50c50e3b0a0042ce32defffd64aafe",
     "grade": true,
     "grade_id": "exercise-1-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `avg_friend_score_map` function\n",
    "\"\"\"\n",
    "\n",
    "# save the original `friends` social network graph \n",
    "# (redundant operation so as to be sure everything will be ok if the original `friends` gets accidentally modified)\n",
    "old_friends = copy.deepcopy(friends)\n",
    "\n",
    "# call the `avg_friend_score_map` function\n",
    "afsm = avg_friend_score_map()\n",
    "# Tests\n",
    "assert_equal(True, np.abs(afsm[\"Rachel Green\"] - 0.7185714285714285) < EPSILON)\n",
    "assert_equal(True, np.abs(afsm[\"Paul Stevens\"] - 0.58) < EPSILON)\n",
    "assert_equal(True, np.abs(afsm[\"Phoebe Buffay\"] - 0.6257142857142857) < EPSILON)\n",
    "\n",
    "# revert to the original `friends` social network graph\n",
    "friends = old_friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "732cccfd5879d88b6909e7254ce00420",
     "grade": false,
     "grade_id": "exercise-1-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.3 (5 points)\n",
    "\n",
    "Implement the function <code><b>top_k_in_degree</b></code>, which takes as input an integer <code><b>k</b></code> and returns the list of top-<code><b>k</b></code> characters having the highest <b>in-degree</b>. The in-degree of a character <code><b>u</b></code> is the number of _other_ characters <code><b>v</b></code> who have <code><b>u</b></code> as friend. The final list to be returned is actually a list of _tuples_ $[(a_1, b_1), \\ldots, (a_k, b_k)]$, where each $a_i$ is the name of a character and $b_i$ is her/his in-degree value. Finally, the list must be sorted in <b>non-decrasing order</b> of $b_i$ and in <b>non-increasing order</b> of $a_i$ (i.e., lexicographically). \n",
    "\n",
    "(**SUGGESTION:** You can do this exercise with one-pass scan over the original <code><b>friends</b></code> dictionary and using an additional dictionary <code><b>in_degree_map</b></code> where storing information about each character's in-degree. Once you have successfully populated that, you can get the list of tuples corresponding to the dictionary by simply invoking <code><b>in_degree_map.items()</b></code>...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "152f20f69aab212eda419bc5ca57391a",
     "grade": false,
     "grade_id": "exercise-1-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the function 'top_k_in_degree' which accepts the parameter 'k'\n",
    "def top_k_in_degree(k):\n",
    "    # Initialize a dictionary 'in_degree_map' where each character's name is a key,\n",
    "    # and its initial in-degree value is 0\n",
    "    in_degree_map = {char: 0 for char in friends.keys()}\n",
    "    \n",
    "    # Iterate over each character and their friends in the 'friends' dictionary\n",
    "    for char, char_friends in friends.items():\n",
    "    \n",
    "    # For each friend of the current character\n",
    "        for friend, _ in char_friends:\n",
    "            \n",
    "            # Increment the in-degree of the friend by 1\n",
    "            in_degree_map[friend] += 1  \n",
    "            \n",
    "             # Sort the 'in_degree_map' by in-degree value in non-decreasing order and then by name in non-increasing order\n",
    "             # 'sorted_list_of_tuples' will be a list of tuples where each tuple contains a character's name and its in-degree\n",
    "            \n",
    "            sorted_list_of_tuples = sorted(in_degree_map.items(), key = lambda x: (-x[1], x[0]))\n",
    "            \n",
    "            \"\"\"sorted(..., key = lambda x: (-x[1], x[0])): This sorts the items of the dictionary. The sorting \n",
    "            is based on a custom key function defined by the lambda expression: lambda x: (-x[1], x[0]) \n",
    "            means that the sorting is primarily based on the in-degree value (x[1]) \n",
    "            in descending order (hence the negative sign) and, in the case of ties,\n",
    "            lexicographically by the character's name (x[0]) in ascending order.\"\"\"\n",
    "            \n",
    "            \n",
    "    return sorted_list_of_tuples[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b35534a480132c815fdaa94f169a379a",
     "grade": true,
     "grade_id": "cell-1-3-test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `top_k_in_degree` function\n",
    "\"\"\"\n",
    "\n",
    "# save the original `friends` social network graph \n",
    "# (redundant operation so as to be sure everything will be ok if the original `friends` gets accidentally modified)\n",
    "old_friends = copy.deepcopy(friends)\n",
    "\n",
    "# Tests\n",
    "assert_equal(5, len(top_k_in_degree(5)))\n",
    "assert_equal('Rachel Green', top_k_in_degree(3)[1][0])\n",
    "assert_equal('Monica Geller', top_k_in_degree(1)[0][0])\n",
    "assert_equal(6, top_k_in_degree(10)[3][1])\n",
    "\n",
    "# revert to the original `friends` social network graph\n",
    "friends = old_friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "87305cc1812eb7d437dc79d35d5ac23c",
     "grade": false,
     "grade_id": "exercise-1-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.4 (6 points)\n",
    "\n",
    "In this exercise, you are asked to implement the function <code><b>path_likelihood</b></code>, which takes as input a list of characters <code><b>character_list</b></code> and returns the likelihood of the path between each element of the input list, <b>providing such a path exists</b>.<br/>\n",
    "The function assumes you will be working with a variant of the original <code><b>friends</b></code> dictionary given at the beginning; more specifically, you will be asked to firstly transform <code><b>friends</b></code> into another dictionary by implementing the function <code><b>markovian_friends</b></code> below. Such a function, transforms the original dictionary in such a way that original friendship scores are considered as probability scores (i.e., a probability distribution of the original friendship scores). For your own convenience, it would be beneficial if <code><b>markovian_friends</b></code> is a dictionary of dictionaries, as follows:\n",
    "$$\n",
    "markovian\\_friends[u] = \\{v: p_{u,v}, w: p_{u,w}, \\ldots, z: p_{u,z}\\} \n",
    "$$\n",
    "In other words, each key <code><b>u</b></code> of <code><b>markovian_friends</b></code> is mapped to another dictionary (rather than a list of tuples, as originally done) whose keys are <code><b>u</b></code>'s friend names and whose values are the normalized friendship scores.<br/>\n",
    "Finally, the path likelihood of a sequence of characters given as input can be computed as follows: it is either the product of all the friendship scores along the path or **0** if no paths exist between the first and the last character of the input sequence.\n",
    "\n",
    "(**NOTE:** You can assume the input sequence is not empty and the first element is a character who is present in the social network.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "49e830b9609f601435077031992ffb57",
     "grade": false,
     "grade_id": "exercise-1-4-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#def markovian_friends():\n",
    "\"\"\"\n",
    "    Return a new dictionary representing the `friends` graph, where friendship scores are considered as probabilities.\n",
    "    In other words, `markovian_friends[u] = {v: p_u,v, w: p_u,w, ..., z: p_u,z}`, where:\n",
    "    p_u,i = the probability of reaching i from u (i.e., the normalized friendship score between u and i).\n",
    "\"\"\"\n",
    "\n",
    "#def path_likelihood(character_list):\n",
    "\"\"\"\n",
    "    Return the path likelihood of the sequence of characters given as input `character_list`.\n",
    "    Let [u1, ..., um] be the input sequence of characters, then the path likelihood is as follows:\n",
    "        - p_u1,u2 * p_u2,u3 * ... * p_um-1,um (if there exists a path connecting u1 to um)\n",
    "        - 0 (if there exists at least two consecutive characters of the sequence (ui, ui+1) which are not directly connected)\n",
    "    (NOTE: `character_list` in not empty and its first element u1 is surely in the social network).\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "def markovian_friends(friends):\n",
    "    markovian = {}\n",
    "    for person, friendships in friends.items():\n",
    "        # Calculate the total friendship score for normalization\n",
    "        total_score = sum(score for _, score in friendships)\n",
    "        # Avoid division by zero by checking if total_score is greater than 0\n",
    "        if total_score > 0:\n",
    "            # Normalize and create a dictionary for each person's friendships\n",
    "            markovian[person] = {friend: score / total_score for friend, score in friendships}\n",
    "    return markovian\n",
    "\n",
    "def path_likelihood(character_list):\n",
    "    # Transform the friends dictionary into the markovian_friends variant\n",
    "    markovian = markovian_friends(friends)\n",
    "    likelihood = 1  # Initialize likelihood as 1 (100%)\n",
    "    for i in range(len(character_list) - 1):\n",
    "        current_char = character_list[i]\n",
    "        next_char = character_list[i + 1]\n",
    "        # If next character is not a friend of current character, or character does not exist, return 0\n",
    "        if next_char not in markovian.get(current_char, {}):\n",
    "            return 0\n",
    "        # Multiply likelihood by the friendship probability\n",
    "        likelihood *= markovian[current_char][next_char]\n",
    "    return likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8a3d44f311dbdd92a2bb29edb3529432",
     "grade": true,
     "grade_id": "exercise-1-4-test",
     "locked": true,
     "points": 7,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `path_likelihood` function\n",
    "\"\"\"\n",
    "\n",
    "# save the original `friends` social network graph \n",
    "# (redundant operation so as to be sure everything will be ok if the original `friends` gets accidentally modified)\n",
    "old_friends = copy.deepcopy(friends)\n",
    "\n",
    "# Tests\n",
    "assert_equal(True, \n",
    "             np.abs(path_likelihood(['Rachel Green', 'Ross Geller']) - 0.1988071570576541) < EPSILON)\n",
    "assert_equal(True, \n",
    "             np.abs(path_likelihood(['Phoebe Buffay', 'Ursula Buffay']) - 0.0022831050228310505) < EPSILON)\n",
    "assert_equal(True, \n",
    "             np.abs(path_likelihood(['Monica Geller', 'Chandler Bing', 'Gunther']) - 0.012784189067790755) < EPSILON)\n",
    "assert_equal(0, \n",
    "             path_likelihood(['Monica Geller', 'Chandler Bing', 'Gunther', 'Ross Geller']))\n",
    "\n",
    "# revert to the original `friends` social network graph\n",
    "friends = old_friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1db87e8d117165548c9344342c6312e0",
     "grade": false,
     "grade_id": "part-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Data Science (15 points)\n",
    "\n",
    "In this part, you will be working with the dataset file <code>**dataset_notype.csv**</code> and a database file named <code>**dataset_type.sqlite**</code>. For a complete description of this data source, please refer to the <code>**README.txt**</code> file included in the archive.\n",
    "In a nutshell, this dataset contains **1781** unique (anonymised) URLs, along with a set of **19 features** and a **binary class label** (<code>**TYPE**</code>), which indicates whether the corresponding URL is malicious (<b>1</b>) or not (<b>0</b>).<br />\n",
    "The cell below is responsible for correctly loading the first part of dataset (without the *type* column) from the <code>**dataset_notype.csv**</code> file. Once this is executed, you can start answering the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "261c4a258d84d3b779229cc8116a962c",
     "grade": false,
     "grade_id": "part-2-required",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded `websites` dataset into a dataframe of size (1781 x 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>URL_LENGTH</th>\n",
       "      <th>NUMBER_SPECIAL_CHARACTERS</th>\n",
       "      <th>CHARSET</th>\n",
       "      <th>SERVER</th>\n",
       "      <th>CONTENT_LENGTH</th>\n",
       "      <th>WHOIS_COUNTRY</th>\n",
       "      <th>WHOIS_STATEPRO</th>\n",
       "      <th>WHOIS_REGDATE</th>\n",
       "      <th>WHOIS_UPDATED_DATE</th>\n",
       "      <th>TCP_CONVERSATION_EXCHANGE</th>\n",
       "      <th>DIST_REMOTE_TCP_PORT</th>\n",
       "      <th>REMOTE_IPS</th>\n",
       "      <th>APP_BYTES</th>\n",
       "      <th>SOURCE_APP_PACKETS</th>\n",
       "      <th>REMOTE_APP_PACKETS</th>\n",
       "      <th>SOURCE_APP_BYTES</th>\n",
       "      <th>REMOTE_APP_BYTES</th>\n",
       "      <th>APP_PACKETS</th>\n",
       "      <th>DNS_QUERY_TIMES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0_109</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>iso-8859-1</td>\n",
       "      <td>nginx</td>\n",
       "      <td>263.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/10/2015 18:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1153</td>\n",
       "      <td>832</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0_2314</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>Apache/2.4.10</td>\n",
       "      <td>15087.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1230</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>1265</td>\n",
       "      <td>1230</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0_911</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>us-ascii</td>\n",
       "      <td>Microsoft-HTTPAPI/2.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0_113</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>iso-8859-1</td>\n",
       "      <td>nginx</td>\n",
       "      <td>162.0</td>\n",
       "      <td>US</td>\n",
       "      <td>AK</td>\n",
       "      <td>7/10/1997 4:00</td>\n",
       "      <td>12/09/2013 0:45</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>3812</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>18784</td>\n",
       "      <td>4380</td>\n",
       "      <td>39</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0_403</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124140.0</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>12/05/1996 0:00</td>\n",
       "      <td>11/04/2017 0:00</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4278</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>129889</td>\n",
       "      <td>4586</td>\n",
       "      <td>61</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL  URL_LENGTH  NUMBER_SPECIAL_CHARACTERS     CHARSET  \\\n",
       "0   M0_109          16                          7  iso-8859-1   \n",
       "1  B0_2314          16                          6       utf-8   \n",
       "2   B0_911          16                          6    us-ascii   \n",
       "3   B0_113          17                          6  iso-8859-1   \n",
       "4   B0_403          17                          6       utf-8   \n",
       "\n",
       "                  SERVER  CONTENT_LENGTH WHOIS_COUNTRY WHOIS_STATEPRO  \\\n",
       "0                  nginx           263.0           NaN            NaN   \n",
       "1          Apache/2.4.10         15087.0           NaN            NaN   \n",
       "2  Microsoft-HTTPAPI/2.0           324.0           NaN            NaN   \n",
       "3                  nginx           162.0            US             AK   \n",
       "4                    NaN        124140.0            US             TX   \n",
       "\n",
       "      WHOIS_REGDATE WHOIS_UPDATED_DATE  TCP_CONVERSATION_EXCHANGE  \\\n",
       "0  10/10/2015 18:21                NaN                          7   \n",
       "1               NaN                NaN                         17   \n",
       "2               NaN                NaN                          0   \n",
       "3    7/10/1997 4:00    12/09/2013 0:45                         31   \n",
       "4   12/05/1996 0:00    11/04/2017 0:00                         57   \n",
       "\n",
       "   DIST_REMOTE_TCP_PORT  REMOTE_IPS  APP_BYTES  SOURCE_APP_PACKETS  \\\n",
       "0                     0           2        700                   9   \n",
       "1                     7           4       1230                  17   \n",
       "2                     0           0          0                   0   \n",
       "3                    22           3       3812                  39   \n",
       "4                     2           5       4278                  61   \n",
       "\n",
       "   REMOTE_APP_PACKETS  SOURCE_APP_BYTES  REMOTE_APP_BYTES  APP_PACKETS  \\\n",
       "0                  10              1153               832            9   \n",
       "1                  19              1265              1230           17   \n",
       "2                   0                 0                 0            0   \n",
       "3                  37             18784              4380           39   \n",
       "4                  62            129889              4586           61   \n",
       "\n",
       "   DNS_QUERY_TIMES  \n",
       "0              2.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              8.0  \n",
       "4              4.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset stored at `DATASET_FILE` using \",\" as field separator and '?' to detect NAs\n",
    "\n",
    "data = pd.read_csv(DATASET_FILE, \n",
    "                   sep=';',\n",
    "                   na_values=['?', 'None'])\n",
    "\n",
    "print(\"Loaded `websites` dataset into a dataframe of size ({} x {})\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "49e03d18e79b9a0b7657537a315f6957",
     "grade": false,
     "grade_id": "exercise-2-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.1 (1 point)\n",
    "\n",
    "Implement the function <code>**get_avg_dns_query_times**</code> below. This takes as input a <code>**pandas.DataFrame**</code> object, and returns the average DNS query times (i.e., <code><b>DNS_QUERY_TIMES</b></code> field) in the dataset.\n",
    "\n",
    "(**NOTE:** Use the mean() function of the pandas package in order to avoid errors in numerical approximations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b51aa01268ef715d439082cb9c19a186",
     "grade": false,
     "grade_id": "exercise-2-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_avg_dns_query_time(data):\n",
    "    \"\"\"\n",
    "    Return the average DNS query times (i.e., `DNS_QUERY_TIMES` field) computed across the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return data['DNS_QUERY_TIMES'].mean()\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "71ee816e1210b513a546c6c1edf2b0a8",
     "grade": true,
     "grade_id": "exercise-2-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `get_avg_dns_query_time` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal(True, get_avg_dns_query_time(data) < 100)\n",
    "assert_equal(True, get_avg_dns_query_time(data) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "252f654c8184c3516ca30e618bd2d347",
     "grade": false,
     "grade_id": "exercise-2-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.2 (1 points)\n",
    "\n",
    "Implement the function <code><b>bytes_per_app_packet</b></code> below. This takes as input a <code>**pandas.DataFrame**</code> object, and creates a new column named <code><b>BYTES_PER_APP_PACKET</b></code> which contains the average bytes per app packet, i.e., it results from the ratio of two columns: <code><b>APP_BYTES</b></code> and <code><b>APP_PACKETS</b></code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>URL_LENGTH</th>\n",
       "      <th>NUMBER_SPECIAL_CHARACTERS</th>\n",
       "      <th>CHARSET</th>\n",
       "      <th>SERVER</th>\n",
       "      <th>CONTENT_LENGTH</th>\n",
       "      <th>WHOIS_COUNTRY</th>\n",
       "      <th>WHOIS_STATEPRO</th>\n",
       "      <th>WHOIS_REGDATE</th>\n",
       "      <th>WHOIS_UPDATED_DATE</th>\n",
       "      <th>TCP_CONVERSATION_EXCHANGE</th>\n",
       "      <th>DIST_REMOTE_TCP_PORT</th>\n",
       "      <th>REMOTE_IPS</th>\n",
       "      <th>APP_BYTES</th>\n",
       "      <th>SOURCE_APP_PACKETS</th>\n",
       "      <th>REMOTE_APP_PACKETS</th>\n",
       "      <th>SOURCE_APP_BYTES</th>\n",
       "      <th>REMOTE_APP_BYTES</th>\n",
       "      <th>APP_PACKETS</th>\n",
       "      <th>DNS_QUERY_TIMES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0_109</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>iso-8859-1</td>\n",
       "      <td>nginx</td>\n",
       "      <td>263.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/10/2015 18:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1153</td>\n",
       "      <td>832</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0_2314</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>Apache/2.4.10</td>\n",
       "      <td>15087.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1230</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>1265</td>\n",
       "      <td>1230</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0_911</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>us-ascii</td>\n",
       "      <td>Microsoft-HTTPAPI/2.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0_113</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>iso-8859-1</td>\n",
       "      <td>nginx</td>\n",
       "      <td>162.0</td>\n",
       "      <td>US</td>\n",
       "      <td>AK</td>\n",
       "      <td>7/10/1997 4:00</td>\n",
       "      <td>12/09/2013 0:45</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>3812</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>18784</td>\n",
       "      <td>4380</td>\n",
       "      <td>39</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0_403</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124140.0</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>12/05/1996 0:00</td>\n",
       "      <td>11/04/2017 0:00</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4278</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>129889</td>\n",
       "      <td>4586</td>\n",
       "      <td>61</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL  URL_LENGTH  NUMBER_SPECIAL_CHARACTERS     CHARSET  \\\n",
       "0   M0_109          16                          7  iso-8859-1   \n",
       "1  B0_2314          16                          6       utf-8   \n",
       "2   B0_911          16                          6    us-ascii   \n",
       "3   B0_113          17                          6  iso-8859-1   \n",
       "4   B0_403          17                          6       utf-8   \n",
       "\n",
       "                  SERVER  CONTENT_LENGTH WHOIS_COUNTRY WHOIS_STATEPRO  \\\n",
       "0                  nginx           263.0           NaN            NaN   \n",
       "1          Apache/2.4.10         15087.0           NaN            NaN   \n",
       "2  Microsoft-HTTPAPI/2.0           324.0           NaN            NaN   \n",
       "3                  nginx           162.0            US             AK   \n",
       "4                    NaN        124140.0            US             TX   \n",
       "\n",
       "      WHOIS_REGDATE WHOIS_UPDATED_DATE  TCP_CONVERSATION_EXCHANGE  \\\n",
       "0  10/10/2015 18:21                NaN                          7   \n",
       "1               NaN                NaN                         17   \n",
       "2               NaN                NaN                          0   \n",
       "3    7/10/1997 4:00    12/09/2013 0:45                         31   \n",
       "4   12/05/1996 0:00    11/04/2017 0:00                         57   \n",
       "\n",
       "   DIST_REMOTE_TCP_PORT  REMOTE_IPS  APP_BYTES  SOURCE_APP_PACKETS  \\\n",
       "0                     0           2        700                   9   \n",
       "1                     7           4       1230                  17   \n",
       "2                     0           0          0                   0   \n",
       "3                    22           3       3812                  39   \n",
       "4                     2           5       4278                  61   \n",
       "\n",
       "   REMOTE_APP_PACKETS  SOURCE_APP_BYTES  REMOTE_APP_BYTES  APP_PACKETS  \\\n",
       "0                  10              1153               832            9   \n",
       "1                  19              1265              1230           17   \n",
       "2                   0                 0                 0            0   \n",
       "3                  37             18784              4380           39   \n",
       "4                  62            129889              4586           61   \n",
       "\n",
       "   DNS_QUERY_TIMES  \n",
       "0              2.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              8.0  \n",
       "4              4.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1f3ac9fc14612eb945e93b6a63b1e33b",
     "grade": false,
     "grade_id": "exercise-2-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def bytes_per_app_packet(data):\n",
    "    \"\"\"\n",
    "    Extend the input dataframe with a new column `BYTES_PER_APP_PACKET`, which contains the average number\n",
    "    of bytes per app packet.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    data['BYTES_PER_APP_PACKET'] = data['APP_BYTES']/data['APP_PACKETS']\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "99ec3f4912125b77a41c98ce28d349e4",
     "grade": true,
     "grade_id": "exercise-2-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `bytes_per_app_packet` function\n",
    "\"\"\"\n",
    "\n",
    "# Call the `bytes_per_app_packet` function on a deep copy of the original dataset\n",
    "data_cpy = data.copy()\n",
    "bytes_per_app_packet(data_cpy)\n",
    "\n",
    "# Tests\n",
    "assert_equal(True, \"BYTES_PER_APP_PACKET\" in data_cpy.columns.tolist())\n",
    "assert_equal(True, np.abs(data_cpy.loc[0,\"BYTES_PER_APP_PACKET\"] - 77.77777777777777) < EPSILON)\n",
    "assert_equal(True, np.abs(data_cpy.loc[5,\"BYTES_PER_APP_PACKET\"] - 81.27272727272727) < EPSILON)\n",
    "assert_equal(True, np.abs(data_cpy.loc[37,\"BYTES_PER_APP_PACKET\"] - 64.875) < EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3 (2 points)\n",
    "\n",
    "Connect to the database *database_type.sqlite* and use an SQL query to select all the data of all the columns contained in the table *data_extendd*. Build a pandas DataFrame with the result set in a varibale named *ext*. Rename the columns of the dataframe with the following names: \"ID\", \"TYPE\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0_10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0_100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0_1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0_1001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  TYPE\n",
       "0     B0_1     0\n",
       "1    B0_10     0\n",
       "2   B0_100     0\n",
       "3  B0_1000     0\n",
       "4  B0_1001     0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# YOUR CODE HERE\n",
    "conn = sqlite3.connect('database_type.sqlite')\n",
    "cur = conn.cursor()\n",
    "query = pd.read_sql_query('SELECT * FROM data_extended', conn)\n",
    "ext = pd.DataFrame(query)\n",
    "ext = ext.rename(columns = {'URL':'ID'})\n",
    "ext.head()\n",
    "#raise NotImplementedError()\n",
    "\n",
    "## THERE IS NO TEST OF CORRECTNESS FOR THIS PART\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.4 (2 points)\n",
    "\n",
    "Join the dataframe *data* created before exercise 2.1 and the dataframe *ext* built in the previous exercise using the column *URL* and *ID* respectively. Save the result in the variable *data_ext* (this dataframe must have both the URL and the ID columns).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1781, 22)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "new_ext = ext.rename(columns = {'ID': 'URL'})\n",
    "data_ext = data.merge(new_ext, on = 'URL')\n",
    "data_ext['ID'] = data_ext['URL']\n",
    "data_ext.shape\n",
    "#raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `dist_plot_age`, `mu` and `sigma`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(True, (data_ext.shape[1] == 22))\n",
    "assert_equal(True, (data_ext['ID'][1] == 'B0_2314'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6fc38088724bc558f606c171992b899a",
     "grade": false,
     "grade_id": "exercise-2-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.3 (4 points)\n",
    "\n",
    "Implement the function <code><b>get_servers_with_largest_avg_malicious_content_length</b></code> below, which takes as input a <code><b>pandas.DataFrame</b></code> and an integer <code><b>k</b></code>, and returns an **ordered list** of <code><b>k</b></code> elements, where each element of is a tuple containing the name of the server (i.e., the <code><b>SERVER</b></code> field) and the **average** of content length (i.e., the <code><b>CONTENT_LENGTH</b></code> field) computed for that server, yet **limited to only malicious web traffic** (i.e., <code><b>TYPE=1</b></code>). The final list shall be ordered by such a computed average (non-ascending) and, within that, lexicographically sorted (non-descending).\n",
    "\n",
    "(**SUGGESTION:** In order to answer this question, you will need to compute the average content length for each server, which must be set to **0** whenever a server is not involved in _any_ malicious traffic. You should use <code><b>math.isnan(val)</b></code> method to test whether average content length <code><b>val</b></code> is NaN...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c92f4d884ae0cd6344860e8f2a1830ef",
     "grade": false,
     "grade_id": "exercise-2-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_servers_with_largest_avg_malicious_content_length(data, k):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "49730932f5780f6ad01482050b2a2425",
     "grade": true,
     "grade_id": "exercise-2-3-test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `get_servers_with_largest_avg_malicious_content_length` function\n",
    "\"\"\"\n",
    "\n",
    "# Call `get_servers_with_largest_avg_malicious_content_length` function\n",
    "data_cpy = data_ext.copy()\n",
    "largest_avg_malicious_content_length = get_servers_with_largest_avg_malicious_content_length(data_cpy, 1000)\n",
    "\n",
    "# Tests\n",
    "assert_equal(238, len(largest_avg_malicious_content_length))\n",
    "assert_equal('marrakesh 1.12.2', largest_avg_malicious_content_length[1][0])\n",
    "assert_equal(9192, largest_avg_malicious_content_length[3][1])\n",
    "assert_equal(('Apache/2.4.23 (Unix) OpenSSL/1.0.1e-fips mod_bwlimited/1.4', 960.75), \n",
    "             largest_avg_malicious_content_length[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "078359a02c74006c5c57c79ed11339e1",
     "grade": false,
     "grade_id": "exercise-2-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.4 (5 points)\n",
    "\n",
    "This exercise is made of **3** main questions, which you can answer independently to each other. Use the newly created *data_ext* dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "596b55fd9e60c60d1c5dbc0375c48fed",
     "grade": false,
     "grade_id": "exercise-2-4-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1 (1 point)\n",
    "\n",
    "Feature <code><b>WHOIS_COUNTRY</b></code> represents a categorical variable which can take on <b>48</b> distinct values.\n",
    "Implement the function <code><b>top_k_countries</b></code> below, which takes as input an integer <code><b>k</b></code> and returns the list of the <code><b>k</b></code>-most frequent countries appearing in the dataset.\n",
    "\n",
    "(**NOTE:** To access the 0-based indices of a <code><b>pandas.Series</b></code> object you can use the <code><b>.index</b></code> property associated to that object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7d6d8dbedd72bd288dfbd10497796f7a",
     "grade": false,
     "grade_id": "exercise-2-4-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def top_k_countries(k):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    list_country = list(set(data_ext['WHOIS_COUNTRY']))\n",
    "    for country in list_country:\n",
    "        val = data_ext.loc[data_ext['WHOIS_COUNTRY'] == country]['WHOIS_COUNTRY'].count()\n",
    "    return \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c7deed4026ac2ecb90782057197404b9",
     "grade": true,
     "grade_id": "exercise-2-4-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/ccyl3brd18x3ytjk5l39tgy00000gn/T/ipykernel_6757/46308348.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'US'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_countries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_countries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_countries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the `top_k_countries`\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal('US', top_k_countries(10)[0])\n",
    "assert_equal('CA', top_k_countries(10)[1])\n",
    "assert_equal('GB', top_k_countries(20)[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9274cea70047eb76cf8b0323d8ce54cb",
     "grade": false,
     "grade_id": "exercise-2-4-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2 (2 points)\n",
    "\n",
    "Assign the correct values to the three variables below, according to the following requests.\n",
    "\n",
    "-  <code><b>nan_charset_ids</b></code> should contain the index of the records of the dataframe whose values of <code><b>CHARSET</b></code> is <code><b>NA</b></code>;\n",
    "\n",
    "-  <code><b>nan_charset_ratio</b></code> should contain the proportion of records (out of all records) of the dataframe whose values of <code><b>CHARSET</b></code> is <code><b>NA</b></code>;\n",
    "\n",
    "-  <code><b>data_cpy</b></code> is a copy of the original dataframe, where where <code><b>NA</b></code> values of <code><b>CHARSET</b></code> will be replaced with the mode (i.e., most frequent value) calculated over non-<code><b>NA</b></code> values.\n",
    "\n",
    "(**NOTE:** Remember that <code><b>mode()</b></code> returns a one-element <code><b>pandas.Series</b></code> object; to get the actual mode value you have to pick the first and only element of that series...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b4e43069158d9c688e300640f5f52d6e",
     "grade": false,
     "grade_id": "exercise-2-4-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/ccyl3brd18x3ytjk5l39tgy00000gn/T/ipykernel_6757/267190025.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nan_charset_ids = None # assign this to the index of records whose values of `CHARSET` is NA\n",
    "nan_charset_ratio = None # assign this to the ratio of NA values of `CHARSET`\n",
    "data_cpy = data_ext.copy() # copy of the original dataframe, where NA values of `CHARSET` will be replaced with the mode\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "db07c31bdbb64bc55cb7941ab90c5c50",
     "grade": true,
     "grade_id": "exercise-2-4-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "False != True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/ccyl3brd18x3ytjk5l39tgy00000gn/T/ipykernel_6757/2136355837.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnan_charset_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnan_charset_ratio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_cpy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/unittest/case.py\u001b[0m in \u001b[0;36massertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \"\"\"\n\u001b[1;32m    828\u001b[0m         \u001b[0massertion_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getAssertEqualityFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0massertion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertNotEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/unittest/case.py\u001b[0m in \u001b[0;36m_baseAssertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0mstandardMsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s != %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_common_shorten_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massertEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: False != True"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `nan_charset_ids`, `nan_charset_ratio`, and `data_cpy`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(False, (nan_charset_ids is None))\n",
    "assert_equal(False, (nan_charset_ratio is None))\n",
    "assert_equal(False, (data_cpy is None))\n",
    "assert_equal(81, nan_charset_ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bb6a131184439e69b1c7774e3f886e70",
     "grade": false,
     "grade_id": "exercise-2-4-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3 (2 points)\n",
    "\n",
    "Plot the distribution of <code><b>CONTENT_LENGHT</b></code> over the two possible classes of traffic (i.e., <code><b>TYPE</b></code>) using <code><b>sns.barplot</b></code> and assign the result of the plot to the variable <code><b>bar_plot</b></code>. \n",
    "\n",
    "In addition to that, assign to the variable <code><b>pearson_r</b></code> the Pearson's correlation coefficient computed between the two random varibles (<code><b>APP_BYTES</b></code> and <code><b>APP_PACKETS</b></code>). This can be computed by calling the <code><b>pearsonr</b></code> scipy's built-in function, which takes as input the two random variables and returns a **pair**: the first item is the _actual_ Pearson's correlation coefficient, whilst the second item is the $p$-value associated to the computed statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f330117e080c6043a73b450d8b376693",
     "grade": false,
     "grade_id": "exercise-2-4-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pg/ccyl3brd18x3ytjk5l39tgy00000gn/T/ipykernel_6757/1277899145.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bar_plot = None # assign this to the outcome of sns.barplot call\n",
    "pearson_r = None # assign this to value of the Pearson's correlation coefficient between APP_BYTES and APP_PACKETS\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d429680de9d2885dc5aaa2aa5a83b8c9",
     "grade": true,
     "grade_id": "exercise-2-4-3-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `bar_plot` and `pearson_r`\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal(False, (bar_plot is None))\n",
    "assert_equal(False, (pearson_r is None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Machine Learning for Students of the Cohort 2019-2020\n",
    "## Extra points if your score is > 22\n",
    "\n",
    "In this part, you will be able to show your machine learning skills! Use the scikit-learn package to import all the subpackages that you need. Please follow the structure indicated in the following steps to train a classifier of your choice. The goal is to predict whether a URL is malicious or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "from sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\"\"\"\n",
    "Extract the feature matrix X from our original DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Similarly, we want to extract the target class column vector y.\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use a simple stratified train/test split\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 123, \n",
    "                                                    stratify = y)\n",
    "\n",
    "print(\"Training Set shape: {}\".format(X_train.shape))\n",
    "print(\"Test Set shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Implement the following code to train a model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Fit the model to the training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Run the following code to implement a function which evaluates the effectiveness of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General function used to assess the quality of predictions\n",
    "in terms of two scores: accuracy and ROC AUC (Area Under the ROC Curve)\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate(true_values, predicted_values):\n",
    "    # Classification Accuracy\n",
    "    print(\"Accuracy = {:.3f}\".\n",
    "          format(accuracy_score(true_values, predicted_values)))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print(\"Area Under the ROC Curve (ROC AUC) = {:.3f}\".\n",
    "          format(roc_auc_score(true_values, predicted_values)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following code to test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the quality of predictions made on the test set\n",
    "print(\"***** Evaluate Performance on Test Set *****\") \n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
