{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "b622aa00a88beb1f25bffa2285e37a9d",
     "deletable": false,
     "editable": false,
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 2022-02-15 Exam\n",
    "\n",
    "### General Instructions:\n",
    "\n",
    "Welcome to the **Python Programming (for Data Science)** exam session! Please, read **carefully** the instructions below before start writing code. \n",
    "\n",
    "This session will last **75 minutes** and is divided into **two parts**: one about \"general\" Python programming and the other about Python programming for Data Science. Each part is made of a set of exercises, which globally accounts for **15** + **15** = **30 points**.\n",
    "At the end of these two parts, there is a part related to Machine Learning which is a three-point bonus if you achieve at least a score of 22 in the first two parts.\n",
    "You will earn all of the points associated to an exercise **if and only if** the answer you provide passes successfully **all** the tests (both those that are visible and those that are hidden to you).\n",
    "<br />\n",
    "\n",
    "To actually write down your implementation, make sure to fill in any place that says <code style=\"color:green\">**_# YOUR CODE HERE_**</code>. Note also that you should **either comment or delete** any <code style=\"color:green\">**raise NotImplementedError()**</code> exception.<br />\n",
    "\n",
    "(**Warning**: If you forget to delete the <code style=\"color:green\">**raise NotImplementedError()**</code> in an exercise, you will automatically lose two points for that exercise even if the code does not raise any error).\n",
    "\n",
    "For this exam session **you will not be allowed** to use any lecture material yet you will be able to access the following APIs:\n",
    "\n",
    "-  [Python](https://docs.python.org/3.6/library/index.html)\n",
    "-  [Numpy](https://docs.scipy.org/doc/numpy-1.13.0/reference/)\n",
    "-  [Scipy](https://docs.scipy.org/doc/scipy-1.0.0/reference/)\n",
    "-  [Pandas](https://pandas.pydata.org/pandas-docs/version/0.22/api.html)\n",
    "-  [Matplotlib](https://matplotlib.org/2.1.1/api/index.html)\n",
    "-  [Seaborn](http://seaborn.pydata.org/api.html)\n",
    "-  [SciKit-Learn](http://scikit-learn.org/stable/)\n",
    "\n",
    "Once you are done, save this notebook and rename it as follows:\n",
    "\n",
    "<code>**YOURUSERNAME_2022-02-15.ipynb**</code>\n",
    "\n",
    "where <code>**YOURUSERNAME**</code> is your actual username. To be consistent, we are expecting your username to be composed by your first name's initial, followed by your full lastname. As an example, in my case this notebook must be saved as <code>**gdinunzio_2022-02-15.ipynb**</code> (Remember to insert an underscore <code>**'_'**</code> between your username and the date).<br />\n",
    "\n",
    "Finally, go back to [Moodle](https://esami.elearning.unipd.it/) and check for the \"**2022-02-15 Python Programming Exam**\" item; there, you will be able to upload your notebook file for grading.\n",
    "\n",
    "Note that there is no limit on the number of submissions; however, be careful when you upload a new version of this notebook because each submission overwrites the previous one. \n",
    "The due date indicated above is **strict**; after that, the system will not accept any more submissions and the latest uploaded notebook will be the one considered for grading.\n",
    "\n",
    "The archive you have downloaded (<code style=\"color:magenta\">**2022-02-15-exam.zip**</code>) is orgaized as follows:\n",
    "\n",
    "<code style=\"color:red\">**2022-02-15-exam**</code> (root)<br />\n",
    "|----<code style=\"color:green\">**2022-02-15.ipynb**</code> (_this_ notebook)<br />\n",
    "|----<code>**corpus.txt**</code> (the text corpus you will be using for answering general Python programming questions)<br />\n",
    "|----<code>**dataset.csv**</code> (the dataset you will be using for answering data science related questions)<br />\n",
    "|----<code>**dataset_extra.sqlite**</code> (the database you will be using for answering data science related questions)<br />\n",
    "|----<code>**README.txt**</code> (a description of the dataset above)\n",
    "\n",
    "<center><h3>... Now, sit back, relax, and do your best!</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "checksum": "886cd1b9260f6b68d2f1aa73a6fce3c5",
     "deletable": false,
     "editable": false,
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Adding the following line, allows Jupyter Notebook to visualize plots\n",
    "# produced by matplotlib directly below the code cell which generated those.\n",
    "#%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from nose.tools import assert_equal\n",
    "from operator import itemgetter\n",
    "\n",
    "EPSILON = .0000001 # tiny tolerance for managing subtle differences resulting from floating point operations\n",
    "\n",
    "TEXT_CORPUS_FILE = \"corpus.txt\"\n",
    "DATASET_FILE = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "ec33bddf64f15258e1d3a4acdcdc65c5",
     "deletable": false,
     "editable": false,
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: General Coding (15 points)\n",
    "\n",
    "For **Part 1**, you will be asked to use the list below - called <code>**corpus**</code> - which contains a list of text documents, where each document is represented by a lowercase string with no punctuation character whatsoever.<br /> \n",
    "Please, execute the cell right below to successfully load those documents into <code>**corpus**</code>, see a few sample documents, and then answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "checksum": "2aed6eb17e131fc815f2f071dd4b6404",
     "deletable": false,
     "editable": false,
     "grade": false,
     "grade_id": "part-1-required",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the first 5 documents loaded out of a total of 2497 documents:\n",
      "\n",
      "architecture of a multislot main memory system for 32 gbps operation\n",
      "rulebased service customization via houdini\n",
      "business policy modeling and enforcement in databases\n",
      "a high speed and high linearity ota in 1v power supply voltage\n",
      "predict towards predicting the runtime of large scale iterative analytics\n"
     ]
    }
   ],
   "source": [
    "# used to replace any punctuation symbol with an empty character ('')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# load each individual document as a lowercase string into the list of strings `corpus`\n",
    "corpus = [doc.strip().lower().translate(translator) for doc in open(TEXT_CORPUS_FILE)]\n",
    "\n",
    "# print out the first 5 documents loaded\n",
    "print(\"The following are the first 5 documents loaded out of a total of {} documents:\\n\".format(len(corpus)))\n",
    "print(\"\\n\".join(corpus[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "79d4252ef11999ebc4c410abe64d64c4",
     "deletable": false,
     "editable": false,
     "grade": false,
     "grade_id": "exercise-1-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.1 (2 point)\n",
    "\n",
    "Implement the function <code>**avg_doc_length**</code>, which returns the **average length** calculated over the documents in the <code>**corpus**</code>.<br />\n",
    "We define the _length_ of a document the number of the tokens which the document string is made of; a _token_ is any substring which is separated from the other by a **whitespace character**, i.e., <code>**\" \"**</code>.\n",
    "\n",
    "(**EXAMPLE:** If the document you are working with is the string <code>**\"I think therefore I am\"**</code>, then the corresponding tokens will be: <code>**\"I\"**</code>, <code>**\"think\"**</code>, <code>**\"therefore\"**</code>, <code>**\"I\"**</code>, and <code>**\"am\"**</code> thereby the length of this document will be **5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.703644373247897"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_doc_length():\n",
    "    \n",
    "    return np.mean([len(d.split(' ')) for d in corpus])\n",
    "\n",
    "        \n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "avg_doc_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `avg_doc_length` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal(True, np.abs(8.70364437- avg_doc_length()) < EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "d25edcc84c6041c78ccda47a761cca46",
     "deletable": false,
     "editable": false,
     "grade": false,
     "grade_id": "exercise-1-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.2 (3 points)\n",
    "\n",
    "Implement the function <code>**is_word_in_doc**</code>, which takes as input a string <code>**word**</code> and and integer <code>**doc_id**</code>, and returns <code>**True**</code> if and only if the token <code>**word**</code> appears in the document <code>**doc_id**</code>. By convention, documents are identified by their index position in the <code>**corpus**</code> list, therefore the first element of the list will correspond to <code>**doc_id = 0**</code>, the second to <code>**doc_id = 1**</code>, and so on and so forth.<br />\n",
    "The function must be _case-insensitive_, meaning that <code>**is_word_in_doc(\"galileo\", 42) = is_word_in_doc(\"Galileo\", 42) = is_word_in_doc(\"GALILEO\", 42)**</code>. Finally, if the input <code>**doc_id**</code> is outside of its valid range $[0, N-1]$ (where $N$ is the total number of documents in the <code>**corpus**</code> list), the function should immediately return <code>**False**</code>.\n",
    "\n",
    "(**NOTE:** Words of documents contained in the <code>**corpus**</code> list are already lowercased, but there is no restiction on the <code>**word**</code> input to the <code>**is_word_in_doc**</code> function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2496"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(corpus) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_in_doc(word, doc_id):\n",
    "    \"\"\"\n",
    "    Return True iff the string `word` appears within the document `doc_id`, False otherwise.\n",
    "    Plus, it returns False whenever `doc_id` is outside of its valid range of values [0, N-1] (N = len(corpus))\n",
    "    \"\"\"\n",
    "    if doc_id < (len(corpus)-1):\n",
    "        lowercase_word = word.lower()\n",
    "        lowercase_doc = corpus[doc_id].lower()\n",
    "        \n",
    "        return lowercase_word in lowercase_doc\n",
    "\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `is_word_in_doc` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal(False, is_word_in_doc(\"runtime\", 5))\n",
    "assert_equal(True, is_word_in_doc(\"STACKED\", 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "d8e893c9b8ee16d63be51c20230343fb",
     "grade": false,
     "grade_id": "exercise-1-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.3 (4 points)\n",
    "\n",
    "Implement the function <code>**doc_stats**</code>, which returns a custom data structure, i.e., dictionary, where each key is a <code>**doc_id**</code> and each value is a tuple containing the <code>**min**</code>, <code>**max**</code>, <code>**mean**</code>, and <code>**standard deviation**</code> (in this very specific order) of the number of characters of the words which _that_ <code>**doc_id**</code> is made of.<br />\n",
    "For example, if the document is <code>\"I have been to Chargoggagoggmanchauggagoggchaubunagungamaugg lake last summer\"</code>, then:\n",
    "-  <code>**min = 1**</code>\n",
    "-  <code>**max = 45**</code>\n",
    "-  <code>**mean = 8.75**</code>\n",
    "-  <code>**std_dev = 13.77**</code>\n",
    "\n",
    "(**NOTE:** The _Chargoggagoggmanchauggagoggchaubunagungamaugg_ lake truly exists, and is located in Webster, Massachussets, USA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_stats():\n",
    "    \"\"\"\n",
    "    Returns a dictionary where each key is a `doc_id` and each value is a tuple containing \n",
    "    the min, max, mean, and standard deviation of the number of characters of each word for that document.\n",
    "    \"\"\"\n",
    "    doc_stats = {}  # This is the variable that needs to be returned\n",
    "    \n",
    "    for doc_id, document in enumerate(corpus):\n",
    "        words = document.split()\n",
    "        word_len = [len(w) for w in words]\n",
    "\n",
    "        min_word = min(word_len)\n",
    "        max_word = max(word_len)\n",
    "        mean_len = statistics.mean(word_len)\n",
    "\n",
    "        n = len(word_len)\n",
    "        if n > 1:\n",
    "            variance = sum((x - mean_len) ** 2 for x in word_len) / n\n",
    "            std_dev = math.sqrt(variance)\n",
    "        else:\n",
    "            std_dev = 0\n",
    "\n",
    "        doc_stats[doc_id] = (min_word, max_word, mean_len, std_dev)\n",
    "\n",
    "    return doc_stats  # Make sure to return doc_stats\n",
    "\n",
    " \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `doc_stats` function\n",
    "\"\"\"\n",
    "\n",
    "# Call off the function implemented above\n",
    "stats = doc_stats()\n",
    "\n",
    "# Tests\n",
    "assert_equal(False, 14 == stats[0][1])\n",
    "assert_equal(9, stats[3][1])\n",
    "assert_equal(True, np.abs(6.818181818 - stats[5][2]) < EPSILON)\n",
    "assert_equal(True, np.abs(2.531797780 - stats[14][3]) < EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "00e8cf2e5af92e93989b02ddd59099b8",
     "grade": false,
     "grade_id": "exercise-1-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.4 (6 points)\n",
    "\n",
    "Implement the function <code>**get_most_similar_docs**</code>, which takes as input a document identifier <code>**doc_i**</code> and an integer <code><b>n</b></code> (<code>**n > 0**</code>), and returns an **ordered list of pairs**, where each pair is as follows: $(doc_j, sim_{i,j}), (j\\neq i)$, i.e., the first element is a document identifier, whilst the second is the value of **Jaccard_similarity** computed between the set of $n$-grams of <code>**doc_i**</code> and <code>**doc_j**</code>. Such a list must be sorted by similarity (in not-ascending order) and by document identifier (in not-descending order).<br />\n",
    "To compute Jaccard similarity between any two documents you firstly have to extract word $n$-grams out of those documents. For example, if the string document is <code>\"I really like python programming\"</code> then:\n",
    "-  **bi-grams** ($n$ = 2): <code>**[(\"I\", \"really\"), (\"really\", \"like\"), (\"like\", \"python\"), (\"python\", \"programming\")]**</code>\n",
    "-  **tri-grams** ($n$ = 3): <code>**[(\"I\", \"really\", \"like\"), (\"really\", \"like\", \"python\"), (\"like\", \"python\", \"programming\")]**</code>\n",
    "-  ...\n",
    "\n",
    "Finally, suppose $A$ and $B$ represents the sets of $n$-grams as extracted from <code>**doc_i**</code> and another document in the corpus <code>**doc_j**</code>, respectively. Then, the Jaccard similarity between $A$ and $B$ is computed as follows:\n",
    "$$\n",
    "J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "**SUGGESTIONS:** Implement the following **two** helper functions: \n",
    "-  <code>**n_grams(doc, n)**</code> which takes as input the string representing a document and an integer <code><b>n</b></code> (<code>**n > 0**</code>), and extracts the $n$-grams from it, $n =$ <code>**n**</code> (pay attention to how the \"sliding window\" should move across the string in order to extract the corresponding substrings...)\n",
    "-  <code>**jaccard_similarity(a, b)**</code> which takes as input two sets <code><b>a</b></code> and <code><b>b</b></code> and returns the Jaccard similarity as specified above (to avoid 0 division error, the function returns 0 if **both** <code><b>a</b></code> and <code><b>b</b></code> are empty.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SUGGESTION: Implement the function `n_grams` below which takes as input a string `doc` representing a document\n",
    "### and an integer n > 0, and returns a list of tuples containing the n-grams of `doc`)\n",
    "\n",
    "    \n",
    "    \n",
    "### SUGGESTION: Implement the function `jaccard_similarity` below which takes as input two sets `a` and `b`\n",
    "### and computes J = |a & b|/|a U b| (to avoid 0 division error, returns 0 if both a and b are empty)\n",
    "\n",
    "def n_grams(doc, n):\n",
    "    \"\"\"\n",
    "    Extracts n-grams from a given document.\n",
    "    \"\"\"\n",
    "    words = doc.split()\n",
    "    return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "def jaccard_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity between two sets.\n",
    "    \"\"\"\n",
    "    if not a and not b:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = len(set(a).intersection(b))\n",
    "    union = len(set(a).union(b))\n",
    "    return intersection / union\n",
    "\n",
    "def get_most_similar_docs(doc_i, n):\n",
    "    \"\"\"\n",
    "    Returns an ordered list of pairs (doc_j, similarity) where doc_j is a document identifier\n",
    "    and similarity is the Jaccard similarity between the n-grams of doc_i and doc_j.\n",
    "    The list is sorted by similarity in non-ascending order and by document identifier in non-descending order.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "\n",
    "    # Extract n-grams for doc_i\n",
    "    ngrams_i = n_grams(corpus[doc_i], n)\n",
    "\n",
    "    # Calculate Jaccard similarity with other documents\n",
    "    for doc_j, doc_content in enumerate(corpus):\n",
    "        if doc_i != doc_j:  # Exclude self-similarity\n",
    "            ngrams_j = n_grams(doc_content, n)\n",
    "            similarity = jaccard_similarity(ngrams_i, ngrams_j)\n",
    "            similarities.append((doc_j, similarity))\n",
    "\n",
    "    # Sort the list by similarity (in non-ascending order) and by document identifier (in non-descending order)\n",
    "    similarities.sort(key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `get_most_similar_docs` function\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(1260, get_most_similar_docs(569, 2)[0][0])\n",
    "assert_equal(True, np.abs(0.0625 - get_most_similar_docs(569, 2)[0][1]) < EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "b35fc80e57c4551152f097e167961b11",
     "grade": false,
     "grade_id": "part-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Data Science (15 points)\n",
    "\n",
    "In this part, you will be working with the dataset file <code>**dataset.csv**</code>. For a complete description of this data source, please refer to the <code>**README.txt**</code> file included in the archive.\n",
    "In a nutshell, this dataset contains **721** unique Pokemons, including their number (ID), name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. Finally, it also shows whether the Pokemon is \"legendary\" or not. The csv file contains the first 11 columns, the database (dataset_extra.sqlite) contains the last 2 features (for a total of 3 columns including the name of the pokemon).<br />\n",
    "The cell below is responsible for correctly loading the dataset from the <code>**dataset.csv**</code> file. Once this is executed, you can start answering the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded `Pokemon` dataset into a dataframe of size (721 x 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>type_1</th>\n",
       "      <th>type_2</th>\n",
       "      <th>total</th>\n",
       "      <th>hp</th>\n",
       "      <th>attack</th>\n",
       "      <th>defense</th>\n",
       "      <th>special_attack</th>\n",
       "      <th>special_defense</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>318</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>405</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>525</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Charmeleon</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405</td>\n",
       "      <td>58</td>\n",
       "      <td>64</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        name type_1  type_2  total  hp  attack  defense  special_attack  \\\n",
       "0   1   Bulbasaur  Grass  Poison    318  45      49       49              65   \n",
       "1   2     Ivysaur  Grass  Poison    405  60      62       63              80   \n",
       "2   3    Venusaur  Grass  Poison    525  80      82       83             100   \n",
       "3   4  Charmander   Fire     NaN    309  39      52       43              60   \n",
       "4   5  Charmeleon   Fire     NaN    405  58      64       58              80   \n",
       "\n",
       "   special_defense  speed  \n",
       "0               65     45  \n",
       "1               80     60  \n",
       "2              100     80  \n",
       "3               50     65  \n",
       "4               65     80  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset stored at `DATASET_FILE` using \",\" as field separator and '?' to detect NAs\n",
    "# and the specified columns as header\n",
    "\n",
    "# column names used as header\n",
    "colnames = ['id', 'name', 'type_1', 'type_2', 'total', 'hp', 'attack', 'defense', \n",
    "            'special_attack', 'special_defense', 'speed']\n",
    "\n",
    "# load dataset\n",
    "data = pd.read_csv(DATASET_FILE, \n",
    "                   sep=';',\n",
    "                   header=0,\n",
    "                   names=colnames,\n",
    "                   na_values='?')\n",
    "\n",
    "# remove any duplicates\n",
    "data = data.drop_duplicates('id', keep='first', inplace=False)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Loaded `Pokemon` dataset into a dataframe of size ({} x {})\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "302f82a7bb2bca90a6c0db0ec0ba7afc",
     "grade": false,
     "grade_id": "exercise-2-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.1 (1 point)\n",
    "\n",
    "Implement the function <code>**get_the_highest_hit_points**</code> below. This takes as input a <code>**pandas.DataFrame**</code> object, and returns the record (i.e., the <code>**pandas.Series**</code>) of the Pokemon with the highest hit points in the collection (i.e., the one with the highest <code>**hp**</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_highest_hit_points(data):\n",
    "    \"\"\"\n",
    "    Returns the record of the slowest Pokemon in the collection (i.e., the one with the lowest speed)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `get_the_slowest` function\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(242, get_the_highest_hit_points(data)['id'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 (2 point)\n",
    "\n",
    "Connect to the database *dataset_extra.sqlite* and use an SQL query to select all the data of the columns *name*, *generation*, and *is_legendary* contained in the table *pokemon_extra*. Build a pandas DataFrame with the result set in a varibale named *poke*. Rename the columns of the dataframe with the following names: \"nameP\", \"generationP\", \"is_legendaryP\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "## THERE IS NO TEST OF CORRECTNESS FOR THIS PART\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3 (3 point)\n",
    "\n",
    "Merge the dataframe *data* created before exercise 2.1 and the dataframe *poke* built in the previous exercise using the column *name*. Save the result in the variable *data*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of merging the two datasets\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(True, (data.shape[0] == 721))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "b71481945beb82fd5328d275ccedbad0",
     "grade": false,
     "grade_id": "exercise-2-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.4 (4 points)\n",
    "\n",
    "Implement the function <code>**attack_stats**</code> below. This takes as input a <code>**pandas.DataFrame**</code> object and returns a tuple containing the min, max, avg, and median value of <code>**attack**</code> feature, yet computed on a _slice_ of the input <code>**pandas.DataFrame**</code>.<br />\n",
    "The sliced dataset represents the subpopulation containing **legendary** Pokemons whose speed is **strictly above the overall average**, and whose hit points ranges in $[50, 79)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "98765ad5e3bde5fad3b12dc270e932fc",
     "grade": false,
     "grade_id": "exercise-2-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def attack_stats(data):\n",
    "    \"\"\"\n",
    "    Returns a tuple containing the min, max, avg, and median value of `attack` feature,\n",
    "    yet limited to a slice of the input DataFrame (data). \n",
    "    In particular, this slice will contain instances referring to legendary Pokemons\n",
    "    whose speed is strictly above the overall average, and whose defense ranges in [52,73).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "75c2bfdf0dba6b1eba69f40cfc76e8a7",
     "grade": true,
     "grade_id": "exercise-2-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `attack_stats` function\n",
    "\"\"\"\n",
    "\n",
    "# Call off `attack_stats` function\n",
    "stats = attack_stats(data)\n",
    "\n",
    "assert_equal(75, stats[0])\n",
    "assert_equal(110.0, stats[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "078359a02c74006c5c57c79ed11339e1",
     "grade": false,
     "grade_id": "exercise-2-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.5 (5 points)\n",
    "\n",
    "This exercise is made of **3** main questions, which you can answer independently to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "8225a2b4d41272330b7bd9d729736176",
     "grade": false,
     "grade_id": "exercise-2-4-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1 (1 point)\n",
    "\n",
    "Feature <code>**generation**</code> represents an ordinal (numerical) variable which can take on <b>6</b> distinct values.\n",
    "Assign to the variable <code>**lowest_generation**</code> below the total number of first-generation Pokemons in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "df343bcf8b03cf51b22c9557f0072a46",
     "grade": false,
     "grade_id": "exercise-2-4-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "lowest_generation = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "e55c1568ee0550ab357762829c755cb8",
     "grade": true,
     "grade_id": "exercise-2-4-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the `lowest_generation`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(True, (lowest_generation == 151))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "1dfcf0c40eb7db2657f30ea678edd9aa",
     "grade": false,
     "grade_id": "exercise-2-4-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2 (1 points)\n",
    "\n",
    "Plot the regression line of <code>**special_attack**</code> ($x$-axis, independent variable) against <code>**special_defense**</code> ($y$-axis, dependent variable) using <code>**sns.regplot**</code> and assign the result of the plot to the variable <code>**reg_plot**</code>. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "checksum": "64b34566504756769710f8fd341297a0",
     "grade": false,
     "grade_id": "exercise-2-4-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "reg_plot = None # assign this to the outcome of sns.regplot call\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "47cfa7c0a060f393931d6917eefa70bb",
     "grade": true,
     "grade_id": "exercise-2-4-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `reg_plot`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(False, (reg_plot == None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "60594ae8967bd7cdd3c31a04fbdeda87",
     "grade": false,
     "grade_id": "exercise-2-4-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3 (3 points)\n",
    "\n",
    "Plot the boxplot of how the variable <code>**speed**</code> is distributed across each of the <b>6</b> generations of Pokemons using <code>**sns.boxplot**</code> function. Then, assign to the list variable <code>**outliers**</code> (initially empty, i.e., <code>**outliers = []**</code>) the values of Pokemon generations which exhibit _any_ outlier (outliers are datapoints that show *extreme* values, these points lie above/below the boxplot). For example, if generations <b>1</b>, <b>3</b>, and <b>5</b> show some outliers then you should make the following assignment: <code>**outliers = [1, 3, 5]**</code> (order **doesn't** matter!).\n",
    "\n",
    "(**NOTE:** If there is no outlier in any Pokemon generation then you should leave the list <code>**outliers**</code> empty as it originally is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "86c7815b415c035db07a080d246d7801",
     "grade": false,
     "grade_id": "exercise-2-4-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "box_plot = None # assign this to the outcome of sns.boxplot call\n",
    "outliers = [] # change this according to the resulting box plot!\n",
    "\n",
    "# Create a Figure containing 1x1 subplots\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8,6))\n",
    "# Box plot 'speed' against 'generation'\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "294459db3e0a60c2aef915f1a4fcf7ba",
     "grade": true,
     "grade_id": "exercise-2-4-3-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `box_plot` and `outliers`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(False, (box_plot == None))\n",
    "assert_equal(False, (outliers == None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Machine Learning \n",
    "## This part is **only** for students of the old cohoort 2019/2020 who decided to take the *old* exam.\n",
    "\n",
    "In this part, you will be able to show your machine learning skills! Use the scikit-learn package to import all the subpackages that you need. Please follow the structure indicated in the following steps to train a classifier of your choice. The goal is to predict whether a Pokemon will be legendary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\"\"\"\n",
    "Extract the feature matrix X from our original DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\"\"\"\n",
    "Similarly, we want to extract the target class column vector y.\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\"\"\"\n",
    "Use a simple stratified train/test split\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 123, \n",
    "                                                    stratify = y)\n",
    "\n",
    "print(\"Training Set shape: {}\".format(X_train.shape))\n",
    "print(\"Test Set shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Implement the following code to train a model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Fit the model to the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Run the following code to implement a function which evaluates the effectiveness of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General function used to assess the quality of predictions\n",
    "in terms of two scores: accuracy and ROC AUC (Area Under the ROC Curve)\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "def evaluate(true_values, predicted_values):\n",
    "    # Classification Accuracy\n",
    "    print(\"Accuracy = {:.3f}\".\n",
    "          format(accuracy_score(true_values, predicted_values)))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print(\"Area Under the ROC Curve (ROC AUC) = {:.3f}\".\n",
    "          format(roc_auc_score(true_values, predicted_values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following code to test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the quality of predictions made on the test set\n",
    "print(\"***** Evaluate Performance on Test Set *****\") \n",
    "\n",
    "### YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
